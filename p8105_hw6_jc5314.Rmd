---
title: "P8105 Homework 6"
author: "Junxian Chen (jc5314)"
date: "11/15/2019"
output: 
  github_document:
    pandoc_args: --webtex
---

```{r, include = FALSE}
library(tidyverse)
library(glmnet)
library(modelr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  message = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)
theme_set(theme_gray(base_size = 10) + theme(legend.position = "bottom"))
```


# Problem 1

Firstly, read in the dataset and do data manipulation.

```{r}
birthweight_dat = 
  read_csv("./data/birthweight.csv") %>% 
  mutate(
    babysex = factor(babysex, levels = c('1', '2')),
    frace = factor(frace, levels = c('1', '2', '3', '4', '8', '9')),
    malform = factor(malform, levels = c('0', '1')),
    mrace = factor(mrace, levels = c('1', '2', '3', '4', '8'))
  )
```

Use the **stepwise regreession method** to do the model selection.

```{r}
model_fit = lm(bwt ~ ., data = birthweight_dat) %>% 
  step(direction = 'backward')

summary(model_fit)
```

**Description of modeling process:** The model was selected by using the stepwise regression method with backward elimination. The process started with all candidate variables included and tested each variable using a chosen model fit criterion. Then, variable whose loss gave the most statistically insignificant deterioration of the model fit was deleted. This process was repeated until no further variables can be deleted without a statistically significant loss of fit.


A plot showing model residuals against fitted values: 

```{r}
birthweight_dat %>% 
  add_predictions(model_fit) %>% 
  add_residuals(model_fit) %>% 
  ggplot(aes(x = pred, y = resid)) +
  geom_point(alpha = .4, size = 2) +
  xlab("Fitted Values") +
  ylab("Residuals")
```

**Comment:** The residuals were mostly centered around 0 but with some outliers on small fitted values.

Model using length at birth and gestational age as predictors (main effects only):

```{r}
model_fit2 = lm(bwt ~ blength + gaweeks, data = birthweight_dat)

model_fit2 %>% broom::tidy() %>% knitr::kable()
```

Model using head circumference, length, sex, and all interactions (including the three-way interaction) between these:

```{r}
model_fit3 = lm(bwt ~ bhead * blength * babysex, data = birthweight_dat)

model_fit3 %>% broom::tidy() %>% knitr::kable()
```

Make comparisons in terms of the cross-validated prediction error.

```{r}
cv_df = 
  crossv_mc(birthweight_dat, 100) %>% 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)
    ) %>% 
  mutate(model1  = map(train, ~lm( bwt ~ babysex + bhead + blength + delwt + fincome + gaweeks + mheight + mrace + parity + ppwt + smoken, data = .x)),
         model2  = map(train, ~lm(bwt ~ blength + gaweeks, data = .x)),
         model3  = map(train, ~lm(bwt ~ bhead * blength * babysex, data = .x))) %>% 
  mutate(rmse_model1 = map2_dbl(model1, test, ~rmse(model = .x, data = .y)),
         rmse_model2 = map2_dbl(model2, test, ~rmse(model = .x, data = .y)),
         rmse_model3 = map2_dbl(model3, test, ~rmse(model = .x, data = .y)))
```

```{r}
cv_df %>% 
  select(starts_with("rmse")) %>% 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") %>% 
  mutate(model = fct_inorder(model)) %>% 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

**Comment:** Based on the above plot, it can be seen that in cross-validation model 1 had the lowest root-mean-square error and model 2 had the largest root-mean-square error. Therefore, we may say that model 1 had the best performance among these three models while model 2 had the worst performance.


# Problem 2

Import data from website:

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

```{r}
boot_straps = weather_df %>% 
  bootstrap(n = 5000) %>% 
  mutate(
    models = map(strap,~ lm(tmax ~ tmin, data = .x) ),
    results = map(models, broom::tidy),
    results_glance = map(models, broom::glance)) %>% 
  select(-strap, -models) %>% 
  unnest(results, results_glance) %>% 
  select(.id, term, estimate, r.squared) %>% 
  pivot_wider(
    names_from = "term",
    values_from = "estimate"
  ) %>% 
  janitor::clean_names() %>% 
  mutate(
    log_coef = log(intercept * tmin)
  )

boot_straps
```

Plot of the distribution of $\hat{r}^2$:

```{r}
boot_straps %>% 
  ggplot(aes(x = r_squared)) +
  geom_density() +
  xlab("Estimated r squared")
```

**Comment:** Based on the plot, the $\hat{r}^2$ followed a normal distribution and had a well bell-shape with a mean value around 0.91. It shows that the overvall values of $\hat{r}^2$ were high and the model had a good performance.

Plot of the distribution of $log(\hat{\beta}_0*\hat{\beta}_1)$:

```{r}
boot_straps %>% 
  ggplot(aes(x = log_coef)) +
  geom_density() +
  xlab("Log(Beta_0 x Beta_1)")
```

**Comment:** Based on the plot, the $log(\hat{\beta}_0*\hat{\beta}_1)$ also followed a normal distribution and formed a bell-shape with a mean value around 2.01.

* 2.5% and 97.5% quantiles for $\hat{r}^2$:

```{r}
ci_r = boot_straps %>% 
  pull(r_squared) %>% 
  quantile(c(0.025, 0.975))

ci_r
```

The 95% confidence interval for $\hat{r}^2$ is (`r round(ci_r[1], 3)`, `r round(ci_r[2], 3)`).

* 2.5% and 97.5% quantiles for $log(\hat{\beta}_0*\hat{\beta}_1)$:

```{r}
ci_log = boot_straps %>% 
  pull(log_coef) %>% 
  quantile(c(0.025, 0.975))

ci_log
```

The 95% confidence interval for $log(\hat{\beta}_0*\hat{\beta}_1)$ is (`r round(ci_log[1], 3)`, `r round(ci_log[2], 3)`).